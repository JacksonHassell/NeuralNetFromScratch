{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Warmup"
      ],
      "metadata": {
        "id": "_kCzvcdAZZF7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVj2l1xwaXyc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f37f0cd-cbf2-4780-994b-22777778de99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0 --------------------------------------------------------\n",
            "0.567188\n",
            "EPOCH: 1 --------------------------------------------------------\n",
            "0.270656\n",
            "EPOCH: 2 --------------------------------------------------------\n",
            "0.1575074\n",
            "EPOCH: 3 --------------------------------------------------------\n",
            "0.11319754\n",
            "EPOCH: 4 --------------------------------------------------------\n",
            "0.094761156\n",
            "EPOCH: 5 --------------------------------------------------------\n",
            "0.08608443\n",
            "EPOCH: 6 --------------------------------------------------------\n",
            "0.08112461\n",
            "EPOCH: 7 --------------------------------------------------------\n",
            "0.07761602\n",
            "EPOCH: 8 --------------------------------------------------------\n",
            "0.074708775\n",
            "EPOCH: 9 --------------------------------------------------------\n",
            "0.07208331\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Y is just a linear combination of X\n",
        "x = torch.rand((100, 10))\n",
        "coefficients = torch.rand(10)\n",
        "y = torch.mv(x, coefficients)\n",
        "\n",
        "weights = torch.rand(10)\n",
        "regularization = .01\n",
        "learning_rate = .001\n",
        "\n",
        "for epoch in range(10):\n",
        "  print(f'EPOCH: {epoch} --------------------------------------------------------')\n",
        "  losses = []\n",
        "  for i in range(len(x)):\n",
        "    pred = torch.dot(weights, x[i].T)\n",
        "    loss = (y[i] - pred)**2\n",
        "    loss_grad = 2 * (pred - y[i])\n",
        "\n",
        "    gradient = (loss_grad * x[i]) + (regularization * weights)\n",
        "    weights = weights - learning_rate * gradient\n",
        "    losses.append(loss)\n",
        "  print(np.mean(losses))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hard Coded Neural Network"
      ],
      "metadata": {
        "id": "jPd9qDT_ZStE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate non linear training data for neural networks\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x = torch.rand((10000, 10))\n",
        "\n",
        "linear_coef = np.random.rand(10)\n",
        "quad_coef = np.random.rand(10)\n",
        "exp_coef = np.random.rand(10)\n",
        "sin_coef = np.random.rand(10)\n",
        "y = (\n",
        "    np.dot(linear_coef, x.T) +\n",
        "    np.dot(quad_coef, x.T**2) +\n",
        "    np.dot(exp_coef, np.exp(x.T)) +\n",
        "    np.dot(sin_coef, np.sin(x.T))\n",
        ")\n",
        "noise = np.random.normal(0, 0.1, len(x))\n",
        "y += noise\n",
        "y = torch.tensor(y, dtype=torch.float).unsqueeze(1)\n",
        "y -= y.min()\n",
        "y /= y.max()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33)"
      ],
      "metadata": {
        "id": "N2hnT43Ct6Y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self):\n",
        "    self.layer1 = torch.rand((10, 10))\n",
        "    self.layer2 = torch.rand((10, 20))\n",
        "    self.layer3 = torch.rand((20, 1))\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "      return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  def sigmoid_derivative(self, x):\n",
        "      return x * (1 - x)\n",
        "\n",
        "  def forward(self, x):\n",
        "    layer1_out = self.sigmoid(x @ self.layer1)\n",
        "    layer2_out = self.sigmoid(layer1_out @ self.layer2)\n",
        "    pred = layer2_out @ self.layer3\n",
        "    return pred\n",
        "\n",
        "  def train(self, x, y):\n",
        "    # Minibatch gradient descent\n",
        "    losses = []\n",
        "    batch_size = 100\n",
        "    for i in range(0, len(x), batch_size):\n",
        "      x_batch, y_batch = x[i:i+batch_size], y[i:i+batch_size]\n",
        "\n",
        "      layer1_out = self.sigmoid(x_batch @ self.layer1) #(N, 10) @ (10, 10) = (N, 10)\n",
        "      layer2_out = self.sigmoid(layer1_out @ self.layer2) #(N, 10) @ (10, 20) = (N, 20)\n",
        "      pred = layer2_out @ self.layer3 #(N, 20) @ (20, 1) = (N, 1)\n",
        "\n",
        "      loss = (y_batch - pred)**2 # N, 1\n",
        "      loss_grad = 2 * (pred - y_batch)/len(y_batch) # N, 1\n",
        "\n",
        "      grad_layer3 = self.sigmoid_derivative(layer2_out).T @ loss_grad + regularization * self.layer3 # (20, N) @ (N, 1) + (20, 1) = (20, 1)\n",
        "      loss_layer2 = loss_grad @ self.layer3.T  # (N, 1) @ (1, 20) = (N, 20)\n",
        "      grad_layer2 = self.sigmoid_derivative(layer1_out).T @ loss_layer2 + regularization * self.layer2 # 10, N @ N, 20 = 10, 20\n",
        "      loss_layer1 = loss_layer2 @ self.layer2.T  # (N, 20) @ (20, 10) = (N, 10)\n",
        "      grad_layer1 = x_batch.T @ loss_layer1 + regularization * self.layer1 # 10, N @ N, 10 = 10, 10\n",
        "\n",
        "      self.layer3 = self.layer3 - learning_rate * grad_layer3\n",
        "      self.layer2 = self.layer2 - learning_rate * grad_layer2\n",
        "      self.layer1 = self.layer1 - learning_rate * grad_layer1\n",
        "\n",
        "      losses.append(loss)\n",
        "\n",
        "    return losses\n",
        "\n",
        "model = NeuralNetwork()\n",
        "regularization = .01\n",
        "learning_rate = .001\n",
        "\n",
        "for epoch in range(10):\n",
        "  print(f'EPOCH: {epoch}')\n",
        "\n",
        "  train_losses = model.train(x_train, y_train)\n",
        "  y_pred = model.forward(x_test)\n",
        "  test_losses = (y_test - y_pred)**2\n",
        "\n",
        "  print(f'Train loss: {np.mean(train_losses)}, test loss: {torch.mean(test_losses)} \\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXv5P3nTDAqW",
        "outputId": "7ef2366e-438b-49ba-fb12-47c4226d6dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n",
            "Train loss: 39.1106071472168, test loss: 13.47390079498291 \n",
            "\n",
            "EPOCH: 1\n",
            "Train loss: 9.59207534790039, test loss: 6.5740485191345215 \n",
            "\n",
            "EPOCH: 2\n",
            "Train loss: 4.807140827178955, test loss: 3.342585802078247 \n",
            "\n",
            "EPOCH: 3\n",
            "Train loss: 2.4535675048828125, test loss: 1.7106475830078125 \n",
            "\n",
            "EPOCH: 4\n",
            "Train loss: 1.2598650455474854, test loss: 0.8809639811515808 \n",
            "\n",
            "EPOCH: 5\n",
            "Train loss: 0.652201771736145, test loss: 0.45865124464035034 \n",
            "\n",
            "EPOCH: 6\n",
            "Train loss: 0.34260228276252747, test loss: 0.24367833137512207 \n",
            "\n",
            "EPOCH: 7\n",
            "Train loss: 0.18483026325702667, test loss: 0.13429103791713715 \n",
            "\n",
            "EPOCH: 8\n",
            "Train loss: 0.1044313982129097, test loss: 0.07866998016834259 \n",
            "\n",
            "EPOCH: 9\n",
            "Train loss: 0.06346756964921951, test loss: 0.050418466329574585 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# General Solution (any combination of layers)"
      ],
      "metadata": {
        "id": "FlNuB9IvZNc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "  def __init__(self, layers, loss_func, loss_func_deriv, batch_size=100):\n",
        "    self.layers = layers\n",
        "    self.loss_func = loss_func\n",
        "    self.loss_func_deriv = loss_func_deriv\n",
        "    self.batch_size = batch_size\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = x\n",
        "    for layer in self.layers:\n",
        "      out = layer.forward(out)\n",
        "    return out\n",
        "\n",
        "  def train(self, x, y):\n",
        "    losses = []\n",
        "    for i in range(0, len(x), self.batch_size):\n",
        "      x_batch, y_batch = x[i:i+self.batch_size], y[i:i+self.batch_size]\n",
        "\n",
        "      y_pred = self.forward(x_batch)\n",
        "      loss = self.loss_func(y_batch, y_pred)\n",
        "      losses.append(torch.mean(loss).squeeze())\n",
        "\n",
        "      loss_grad = self.loss_func_deriv(y_batch, y_pred)\n",
        "      for layer in reversed(self.layers):\n",
        "        loss_grad = layer.backpropagate(loss_grad)\n",
        "\n",
        "    return losses\n",
        "\n",
        "class FF_Layer:\n",
        "  def __init__(self, inp_shape, out_shape):\n",
        "    self.weights = torch.rand((inp_shape, out_shape)) - 0.5\n",
        "    self.bias = torch.rand((1, out_shape))\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.input = x\n",
        "    self.output = self.input @ self.weights + self.bias # (n, inp_shape) @ (inp_shape, out_shape) = (n, out_shape)\n",
        "    return self.output\n",
        "\n",
        "  def backpropagate(self, loss):\n",
        "    gradient = self.input.T @ loss + regularization * self.weights # (inp_shape, n) @ (n, out_shape) = (inp_shape, out_shape)\n",
        "    self.weights -= learning_rate * gradient\n",
        "    self.bias -= learning_rate * (torch.mean(loss, axis=0) * self.bias)\n",
        "    return loss @ self.weights.T # (n, out_shape) @ (out_shape, inp_shape) = (n, inp_shape)\n",
        "\n",
        "class Sigmoid_Activation:\n",
        "  def forward(self, x):\n",
        "    self.input = x\n",
        "    self.output = 1 / (1 + np.exp(-x))\n",
        "    return self.output\n",
        "\n",
        "  def backpropagate(self, loss):\n",
        "    return loss * (self.output * (1 - self.output))\n",
        "\n",
        "class Relu_Activation:\n",
        "  def forward(self, x):\n",
        "    self.input = x\n",
        "    self.output = torch.maximum(x, torch.tensor(0.0))\n",
        "    return self.output\n",
        "\n",
        "  def backpropagate(self, loss):\n",
        "    return loss * (self.output > 0).float()\n",
        "\n",
        "def mean_squared_error(y_true, y_pred):\n",
        "  return (y_true - y_pred)**2\n",
        "\n",
        "def mean_squared_error_deriv(y_true, y_pred):\n",
        "  return 2 * (y_pred - y_true)/len(y_true)\n",
        "\n",
        "model = NeuralNetwork([\n",
        "    FF_Layer(10, 20),\n",
        "    Sigmoid_Activation(),\n",
        "    FF_Layer(20, 50),\n",
        "    Relu_Activation(),\n",
        "    FF_Layer(50, 10),\n",
        "    Relu_Activation(),\n",
        "    FF_Layer(10, 1),\n",
        "], mean_squared_error, mean_squared_error_deriv)\n",
        "\n",
        "regularization = .01\n",
        "learning_rate = .0001\n",
        "\n",
        "for epoch in range(10):\n",
        "  print(f'EPOCH: {epoch}')\n",
        "\n",
        "  train_losses = model.train(x_train, y_train)\n",
        "  y_pred = model.forward(x_test)\n",
        "  test_losses = (y_test - y_pred)**2\n",
        "\n",
        "  print(f'Train loss: {np.mean(train_losses)}, test loss: {torch.mean(test_losses)} \\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EziS4SszqcfQ",
        "outputId": "1d0089a7-8a3e-4294-bd9e-1df9eeff5e61"
      },
      "execution_count": 453,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH: 0\n",
            "Train loss: 0.3687610328197479, test loss: 0.23215410113334656 \n",
            "\n",
            "EPOCH: 1\n",
            "Train loss: 0.1660536229610443, test loss: 0.11483999341726303 \n",
            "\n",
            "EPOCH: 2\n",
            "Train loss: 0.08878140151500702, test loss: 0.06913238018751144 \n",
            "\n",
            "EPOCH: 3\n",
            "Train loss: 0.058044079691171646, test loss: 0.05080677941441536 \n",
            "\n",
            "EPOCH: 4\n",
            "Train loss: 0.045444831252098083, test loss: 0.04326576367020607 \n",
            "\n",
            "EPOCH: 5\n",
            "Train loss: 0.04011203348636627, test loss: 0.04006524011492729 \n",
            "\n",
            "EPOCH: 6\n",
            "Train loss: 0.03775382414460182, test loss: 0.038642123341560364 \n",
            "\n",
            "EPOCH: 7\n",
            "Train loss: 0.036650799214839935, test loss: 0.037969764322042465 \n",
            "\n",
            "EPOCH: 8\n",
            "Train loss: 0.036090053617954254, test loss: 0.0376136414706707 \n",
            "\n",
            "EPOCH: 9\n",
            "Train loss: 0.035769056528806686, test loss: 0.03739610314369202 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "55hdB6DER-Ix"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}